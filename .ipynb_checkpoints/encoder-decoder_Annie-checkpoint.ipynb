{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder RNN\n",
    "\n",
    "\n",
    "This is the entire training and evaluation pipeline for our encoder-decoder RNN. Note that in this notebook, utterance=input and response=output. Cells need to be run one at a time in order.\n",
    "\n",
    "Please go through every line! In particular, try tracing what would happen to line 1 of the data file through the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import re\n",
    "import os, sys, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Don't run these imports on your local machine!\n",
    "import tensorflow as tf\n",
    "#Keras imports\n",
    "from keras.layers import LSTM, Dense, Activation, Input\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "Our preprocessing method opens our data file and separates each line into pairs of utterances and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Preprocessing Methods\n",
    "######################\n",
    "\n",
    "##### Load the raw dataset #####\n",
    "#This method opens the raw text file, reads the lines, and closes the file.\n",
    "def load_data(filename):\n",
    "    file = open(filename, mode=\"rt\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "##### Split data into utterance-response pairs #####\n",
    "#This method splits the dataset into lines, and for each line, we create a dictionary.\n",
    "#The dictionary key is the utterance (A), and the value is the response (B)\n",
    "#For the utterance and response, the speech-tag and actual utterance is tab separated.\n",
    "#We add each set of utterance-response pairs to an array called pairs.\n",
    "def split_to_pairs(data):\n",
    "    lines = data.split(\"\\n\")\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        tokens = line.split(\"\\t\")\n",
    "        utterance = tokens[0] + \"\\t\" + tokens[1]\n",
    "        response = tokens[2] + \"\\t\" + tokens[3]\n",
    "        pairs.append([utterance, response])\n",
    "    return pairs\n",
    "\n",
    "##### Clean the data ######\n",
    "#Optionally, we could make all words lowercase, remove punctuation, etc.\n",
    "#I'm going to just leave the dataset in its native form and see how it does for now.\n",
    "#This method essentially just reorganizes the data into a 2D array, where each row holds:\n",
    "# [utterance, response]\n",
    "def clean_data(pairs):\n",
    "    cleaned_data = list()\n",
    "    for pair in pairs:\n",
    "        clean_pair = list()\n",
    "        for utt in pair:\n",
    "            clean_pair.append(utt)\n",
    "        cleaned_data.append(clean_pair)\n",
    "    return array(cleaned_data)\n",
    "\n",
    "##### Save pairs to file #####\n",
    "def save_pairs(pairs, new_filename):\n",
    "    dump(pairs, open(new_filename, \"wb\"))\n",
    "    print(\"Saved: %s\" % new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: utt-resp.pkl\n",
      "[fp\tand I'm calling from Garland, Texas.] => [b\tYeah,]\n",
      "[co^t\tso. anyway, let me press one.] => [aa\tOkay .]\n",
      "[sd\tand, it was an experience that I won't do again .] => [qw\tHow big a family do you have?]\n",
      "[sd\tWe saw people we hadn't see in a while] => [qy\tDid you have people coming from far away?]\n",
      "[sd(^q)\tand we're going, my gosh.] => [sv\tWell you have]\n",
      "[b\tYeah.] => [sv\tAnd if, they come from far away, they take it more seriously]\n",
      "[aa\tI think you're right.] => [b\tYeah.]\n",
      "[b\tYeah.] => [sd\tMy family's not very big]\n",
      "[qw^d\tYour family's from where?] => [sd\tWell, I have a, a brother lives in Indianapolis, a sister lives in Chicago, and my folks live back in Buffalo, New York.]\n",
      "[ba\tno.] => [sd\tI guess we have reunions about once a year or so.]\n",
      "[b\tUh huh.] => [sd\tWe got together over Christmas.]\n",
      "[ba\tthose are nice.] => [aa\tYeah,]\n",
      "[sv\tAnd I'm sure they're a lot more organized too, because they've done it before.] => [aa\tYeah,]\n",
      "[ba\tThat's great.] => [sd\tand every year everyone asks when is it happening and where is it happening]\n",
      "[b\tYeah.] => [sd\tand they just plan on being here.]\n",
      "[ba\twell that's great.] => [b\tYeah,]\n",
      "[b\tOh.] => [sd\tSo it was good, it, it worked out very well.]\n",
      "[b\tUh huh.] => [sd\tOne interesting thing that they did at the time is they videotaped the whole shebang it made a nice remembrance of the entire party.]\n",
      "[bf\tmore people want to come back again.] => [aa\tYeah,]\n",
      "[ba\tthat's good.] => [sd\tSo, so that works pretty well that way.]\n",
      "[sd\tand, it sounds really neat, it really does.] => [aa\tYeah,]\n",
      "[b\tYeah.] => [sd\tI don't know how you would go about, getting them together,]\n",
      "[sd\tand they have a really good for the whole weekend.] => [b\tYeah,]\n",
      "[qy\tso, do you own a P C?] => [nn\tno,]\n",
      "[qy^d\tBut you have one at work.] => [ny\tYes,]\n",
      "[sd\tsee I'm doing my, Master's in Computer Science and Computer Engineering.] => [b\tUh huh.]\n",
      "[sd\tbut I have to use them, like especially during my undergrad.] => [b\tRight.]\n",
      "[sv\tYou know, like it's easier for you to go to and run a program, you know, through the disk, because, the grader can do it at home.] => [b\tUh huh.]\n",
      "[sv\tand you have to have so many people sharing the same data, that you can't use personal computers, so you have to use, you know, a main frame.] => [b\tUh huh.]\n",
      "[bk\tokay.] => [sd\tAnd, that's been my situation, is that, that way I can get in, access our, computers that I have up here and, you know, do work from home.]\n",
      "[qy\tBut does it have, like, a disk drive?] => [ny\tyeah.]\n",
      "[sd\tbecause, the ones that we use, you know, are like UNIX base systems.] => [b\tRight.]\n",
      "[sd\tAnd, so they don't have a disk drive.] => [b\tRight.]\n",
      "[sd\tthe only way that you can do it is through a modem.] => [b\tRight.]\n",
      "[sd\tAnd, you know, you just do it that way.] => [b\tYeah.]\n",
      "[sd\tyou can't store it anywhere.] => [aa\tRight.]\n",
      "[bk\tokay.] => [b\tUh huh,]\n",
      "[bf\tAnd then you also have to do all your grading on the P C.] => [sd\tWell, that's the really neat thing, I teach in the continuing education classes, so I don't have to have any grades, no grade books, so that's great .]\n",
      "[bk\tokay.] => [o\tBut, no,]\n",
      "[b\tYeah.] => [sd\tso I think it's extremely helpful and very useful.]\n",
      "[sd\tWell, for us you know, it's like for doing like, you know, like resumes, and, presentations.] => [b\tUh huh.]\n",
      "[sd\tWe use like for example, a Mackintosh, which is a lot easier for graphics, than you know, the I B M P C's or anything compatible with that.] => [b\tYeah.]\n",
      "[sd\tdue to the fact that, well, you know, I haven't tested the P S two yet.] => [b\tUh huh.]\n",
      "[sd\tbut I don't know if the software is as easily, you know, like you can manage it a lot easier than the old one.] => [b\tRight.]\n",
      "[sd\tThe old one you had to go pick a line, use little arrows to go onto the screen and check where you wanted to start, and where, you know, with the mouse you do it, you know, like, a hundred times faster.] => [b\tRight.]\n",
      "[sd\tand that's the way it's going to print out.] => [b\tUh huh.]\n",
      "[sd\tAnd, especially if you have a laser printer, it's going to print out the same way as it's on the screen.] => [b\tUh huh.]\n",
      "[sd\tAnd, so, you know, with the I B M what would happen is, since the software that I had was, it was basically, you know, you only see part of the page.] => [b\tUh huh.]\n",
      "[sd\tAnd, so, the whole page you never can actually see it,] => [b\tOh.]\n",
      "[sd\tand, you know, it's like, every time that you have to do something it's really a pain.] => [ba\tWhat a hassle,]\n",
      "[sd\tit's like if you have to do any statistical data it can be easily represented on a P C.] => [b\tUh huh.]\n",
      "[sd\tYou know, like years back when you didn't have that you would have to map out all these numbers.] => [b\tYeah.]\n",
      "[sd\tand get a graph which you weren't sure if it was okay or not, you know.] => [b\tRight.]\n",
      "[sd\tBut, with a new system I can calculate everything so fast, you know, like for spread sheets.] => [b\tUh huh.]\n",
      "[sv\tYou can see what the trend is over the years.] => [b\tYeah.]\n",
      "[b\tYeah,] => [sv\tthey have really simplified things.]\n",
      "[sd\tand the other one I had the program running. so if there was ever a mistake, I could easily check, you know,] => [b\tRight.]\n",
      "[sd(^q)\tI could look at the program and say, this is where I made the error.] => [b\tRight.]\n",
      "[sd(^q)\tInstead of saying, where did I make the error?] => [b\tUh huh.]\n",
      "[sd\tthe old ones you had to go out of your program, well load up the program again, in this case.] => [b\tUh huh.]\n",
      "[sd\tAfter you load it up change it, hope that's right, get out of that, run the program, run, uh huh, as long as it took, and then go back and see if that worked or not.] => [b\tYes.]\n",
      "[sd\tand say it messed up in line fifty four.] => [b\tYeah.]\n",
      "[sd\tand you can see where it messed up, because, you know it's like in the old computers, the ones that, we're using here a couple of years ago, you would always have to have a printout, every time that you ran your program, * end of slash unit? you would need a printout because everything else was erased in the background. / * should have been new slash unit] => [b\tUh huh.]\n",
      "[qy\tdid, did you learn it in computer science?] => [ny\tyeah,]\n",
      "[qw\tAnd, when was this.] => [sd\tI graduated in eighty six.]\n",
      "[bk\tokay.] => [b\tYeah.]\n",
      "[sd\tI know because, all I know is that when I came here in eighty seven, it was the last year to put all your punch cards in.] => [^2\tThe cards.]\n",
      "[sd(^q)\tand they go, my God, they're still using this.] => [b\tYes.]\n",
      "[sd\tand, you know , You can get a hard copy of it,] => [ba\tWow,]\n",
      "[sd\tand that's about it.] => [b\tUh huh,]\n",
      "[sd\tThey said that, if the auto industry would have kept the same trend as the computer industry has ever since, you know, it started, they said that, cars would cost two dollars] => [b\tUh huh.]\n",
      "[sd\tand they would run forever.] => [sv\tthat would be great.]\n",
      "[sd(^q)\tit's like, yeah, you know, when your, car runs out of gas just throw it away.] => [b\tUh huh.]\n",
      "[sv\tAnd, that's the way it would be.] => [ba\tWow.]\n",
      "[sd\tan I B M P C in like in nineteen eighty one it would cost you five thousand dollars,] => [aa\tsure,]\n",
      "[sd\tand now you can get it, you know, like for, one thousand dollars, you know, because of the parts basically.] => [aa\tRight.]\n",
      "[sd(^q)\tthey're not charging you, you know, over pricing, it because it's like, if we sell it for less, you know, it's like we're losing money, you know.] => [b\tUh huh.]\n",
      "[sd(^q)\tIt's like we want to sell it to break even at least.] => [b\tUh huh.]\n",
      "[sd\tAnd, I think they stopped producing the I B M P C.] => [b\tUh huh.]\n",
      "[sd\tand the, P C Junior was a total failure, to them.] => [b\tOh.]\n",
      "[sd(^q)\tit's like, sure bring out. N , into the market something that's smaller when nobody uses anything that's smaller than, you know, this, you know.] => [b\tYeah.]\n",
      "[sd\tand, Mackintosh took a lot of the market from a lot of schools.] => [b\tYeah.]\n",
      "[sv\tyou can work with it a lot easier.] => [aa\tYeah.]\n",
      "[ad\twell, I'll leave you back to your work.] => [aa\tOkay.]\n",
      "[fx\tAnd, have a good lunch.] => [ft\tThanks.]\n",
      "[qo\tHow about yourself?] => [b\tYeah.]\n",
      "[aa\tTrue.] => [sd\twe have a one year old in the house]\n",
      "[b*\tHuh uh.] => [sv\tI have to say, during the war the American public was probably flooded with too much information]\n",
      "[sv\tmy personal opinion is that the various network news medias were trying to interpret the news to the best of their abilities since they were not very well informed by the military.] => [aa\tTrue.]\n",
      "[sd\tNow, I can appreciate that, especially after what happened in Vietnam.] => [aa\tYeah,]\n",
      "[sv\ttoday's, means of communication, a newscaster could very well giveaway a piece of top secret information. And, pat himself on the back for, making a good scoop.] => [b*\tHuh uh.]\n",
      "[sd\tI find T News very enlightening, too.] => [b\tYeah.]\n",
      "[sd\tOf course, with that goes along with the stock price of T I.] => [aa\tYeah.]\n",
      "[sd\tOf course, T NEWS is more or less headlines,] => [b*\tHuh uh.]\n",
      "[sd\tI don't think much of their sports coverage.] => [aa\tYeah]\n",
      "[sd\tOnly give the scores.] => [b\tYeah.]\n",
      "[sd\toutside of pro football, I'm not too interested in other games, other sports, anyway.] => [aa\tHuh uh.]\n",
      "[sv\tI think they do a pretty good job of selecting the most important items.] => [aa\tYeah.]\n",
      "[b*\tHuh uh.] => [sv\tbut you need to continue to read, so you people learn to read, you know .]\n",
      "[sd\tand I know what it takes to get some of those little bits and pieces of news on the air,] => [b*\tHuh uh.]\n"
     ]
    }
   ],
   "source": [
    "#Run preprocessing\n",
    "filename = \"clean_dataset.txt\"\n",
    "data = load_data(filename)\n",
    "pairs = split_to_pairs(data)\n",
    "clean_pairs = clean_data(pairs)\n",
    "save_pairs(clean_pairs, \"utt-resp.pkl\")\n",
    "\n",
    "#Check our dataset\n",
    "#you should see:\n",
    "# [fp\tand I'm calling from Garland, Texas.] => [b\tYeah,], etc.\n",
    "for i in range(100):\n",
    "     print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets and Split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Load dataset methods\n",
    "######################\n",
    "def load_sentences(filename):\n",
    "    return load(open(filename, \"rb\"))\n",
    "\n",
    "def save_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, \"wb\"))\n",
    "    print(\"Saved: %s\" % filename)\n",
    "    \n",
    "def split_dataset(dataset, num_sentences):\n",
    "    split = int(round(num_sentences*0.9))\n",
    "    train = dataset[:split]\n",
    "    test = dataset[split:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: \n",
      "46464\n",
      "Saved: utt-resp-both.pkl\n",
      "Saved: utt-resp-train.pkl\n",
      "Saved: utt-resp-train.pkl\n"
     ]
    }
   ],
   "source": [
    "#For testing purposes, you can change n_sentences to a smaller number.\n",
    "raw_dataset = load_sentences(\"utt-resp.pkl\")\n",
    "n_sentences = len(raw_dataset)\n",
    "print(\"Number of training pairs: \") \n",
    "print(n_sentences)\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "shuffle(dataset)\n",
    "train, test = split_dataset(dataset, n_sentences)\n",
    "save_sentences(dataset, \"utt-resp-both.pkl\")\n",
    "save_sentences(train, \"utt-resp-train.pkl\")\n",
    "save_sentences(test, \"utt-resp-train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Tokenize\n",
    "\n",
    "First, we load the datasets using our load_sentences method from above. We are going to load the full dataset (so we can calculate vocab and max_length sizes), and the train and test data.\n",
    "\n",
    "Next, we tokenize the data. Tokenization is the process of mapping words to integers. We are actually going to create separate tokenizers for our input and response data. Why? Because right now, that makes the code run. We can experiment with using one tokenizer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Load data\n",
    "######################\n",
    "dataset = load_sentences(\"utt-resp-both.pkl\")\n",
    "train = load_sentences(\"utt-resp-train.pkl\")\n",
    "test = load_sentences(\"utt-resp-train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Tokenizer methods\n",
    "######################\n",
    "#create and fit a tokenizer on the given lines\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "#get the max length of all phrases\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Tokenize\n",
    "######################\n",
    "#create tokenizers\n",
    "utterance_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "response_tokenizer = create_tokenizer(dataset[: 1])\n",
    "#define vocabulary sizes\n",
    "utterance_vocab_size = len(utterance_tokenizer.word_index) + 1\n",
    "response_vocab_size = len(response_tokenizer.word_index) + 1\n",
    "#define max_lengths\n",
    "utterance_length = max_length(dataset[:, 0])\n",
    "response_length = max_length(dataset[:, 1])\n",
    "#print some statistics\n",
    "print(\"Utterance vocabulary size: %d\" % utterance_vocab_size)\n",
    "print(\"Utterance max length: %d\" % utterance_length)\n",
    "print(\"Response vocabulary size: %d\" % response_vocab_size)\n",
    "print(\"Utterance max length: %d\" % response_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "We need to encode each utterance-response sequence to integers, and pad each encoding to the maximum phrase length (so that every sequence of encoded integers is the same length).\n",
    "\n",
    "We need the encodings to be the same length because we are going to use a word embedding for the input sequences and one hot encode the output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Encoding methods\n",
    "######################\n",
    "#this method encodes the lines and pads them to the max length\n",
    "def encode_input(tokenizer, length, lines):\n",
    "    encoding = tokenizer.texts_to_sequences(lines)\n",
    "    encoding = pad_sequences(encoding, maxlen=length, padding=\"post\")\n",
    "    return encoding\n",
    "#this method one-hot encodes the output (responses). \n",
    "#we do this because we want the model to predict the probability of each word in the vocabulary as an output.\n",
    "def encode_output(sequences, vocab_size):\n",
    "    output_list = list()\n",
    "    for sequence in sequences:\n",
    "        #to_categorical converts a class vector (integers) to binary class matrix\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        output_list.append(encoded)\n",
    "    output = array(output_list)\n",
    "    #Reshapes an output to a certain shape (the parameter for reshape is a tuple of integers)\n",
    "    output = output.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Encode data\n",
    "######################\n",
    "#training data\n",
    "train_utterance = encode_input(utterance_tokenizer, utterance_length, train[:, 1])\n",
    "train_response = encode_input(response_tokenizer, response_length, train[:, 0])\n",
    "train_response = encode_output(train_response, response_vocab_size)\n",
    "#test data\n",
    "test_utterance = encode_input(utterance_tokenizer, utterance_length, test[:, 1])\n",
    "test_response = encode_input(response_tokenizer, response_length, test[:, 0])\n",
    "test_response = encode_output(test_response, response_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "We will create an encoder-decoder LSTM.\n",
    "\n",
    "### What is a timestep?\n",
    "\n",
    "A timestep is a Keras thing. Here is the formal definition:\n",
    "\n",
    "The specified number of timesteps defines the number of input variables (X) used to predict the next time step (y).\n",
    "\n",
    "So, basically:\n",
    "A timestep is the \"memory\" of an LSTM- it's many inputs we are remembering (I think). In this case, we are using the max_length of an utterance/response as our timestep. This means that for every predicted word, we are taking into account every other word we have predicted so far. Likewise, when we train, we are learning weights for a word based on every previous word in a sentence (this is what we want for an encoder-decoder model!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Methods to create model\n",
    "######################\n",
    "#this method creates a model based on the given inputs.\n",
    "def create_model(input_vocab, output_vocab, input_timesteps, output_timesteps, n_units):\n",
    "    model = Sequential() #we are doing seq2seq \n",
    "    model.add(Embedding(input_vocab, n_units, input_length=input_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(output_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(output_vocab, activation=\"softmax\")))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Create and compile model\n",
    "######################\n",
    "#We can change the number of hidden units (right now its 256)\n",
    "#increasing the number of hidden units will increase performance and training time\n",
    "#We can change the loss function (right now its categorical_crossentropy)\n",
    "#I also create a file called model.png that shows the shape of the model\n",
    "#I thought we might want to use the image for our final presentation :)\n",
    "model = create_model(utterance_vocab_size, response_vocab_size, utterance_length, response_length, 256)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "print(model.summary())\n",
    "plot_model(model, to_file=\"model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model \n",
    "\n",
    "Right now I'm using 30 epochs and a batch_size of 64. We can always up the number of epochs if we aren't getting good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename= \"model.test1\"\n",
    "checkpoint = ModelCheckpoint(filename, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "model.fit(train_utterance, train_response, epochs=30, batch_size=64, validation_data=(test_utterance, test_response), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reload the datasets (just in case)\n",
    "dataset = load_sentences(\"utt-resp-both.pkl\")\n",
    "train = load_sentences(\"utt-resp-train.pkl\")\n",
    "test = load_sentences(\"utt-resp-train.pkl\")\n",
    "#create tokenizers\n",
    "utterance_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "response_tokenizer = create_tokenizer(dataset[: 1])\n",
    "#define vocabulary sizes\n",
    "utterance_vocab_size = len(utterance_tokenizer.word_index) + 1\n",
    "response_vocab_size = len(response_tokenizer.word_index) + 1\n",
    "#define max_lengths\n",
    "utterance_length = max_length(dataset[:, 0])\n",
    "response_length = max_length(dataset[:, 1])\n",
    "#datasets\n",
    "train_utt = encode_sequences(utterance_tokenizer, utterance_length, train[:, 1])\n",
    "test_utt = encode_sequences(utterance_tokenizer, utterance_length, train[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Evaluation methods\n",
    "######################\n",
    "#reverse-lookup a word in the tokenizer \n",
    "def get_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "#we will need to perform this reverse-lookup for every word in a predicted sequence\n",
    "#this method returns the prediction in words (not integers)\n",
    "def get_prediction(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = get_word(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return \" \".join(target)\n",
    "#we need to repeat the prediction for every utterance in the test dataset\n",
    "#we then compare our prediction to the actual response\n",
    "#I'm using a BLEU score to compare these quantitatively, but if we get a low BLEU score I wouldn't be surprised.\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = get_prediction(model, utterance_tokenizer, source)\n",
    "        raw_target, raw_source = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    \t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Evaluate\n",
    "######################\n",
    "model = load_model(\"model.test1\")\n",
    "#evalute on training data (this should be pretty good)\n",
    "evaluate_model(model, utterance_tokenizer, train_utt, train)\n",
    "#evaluate on test data\n",
    "evalute_model(model, utterance_tokenizer, test_utt, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
