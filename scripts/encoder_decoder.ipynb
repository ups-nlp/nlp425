{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Importing essential libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from keras.layers import LSTM, Dense, Activation, Input\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os, sys, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Preprocessing\n",
    "#load text t\n",
    "filename = \"../resources/clean_dataset.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "filename = \"../resources/source_data.txt\"\n",
    "input_text = open(filename).read()\n",
    "\n",
    "filename = \"../resources/target_data.txt\"\n",
    "output_text = open(filename).read()\n",
    "\n",
    "\n",
    "#Splitting text into a word per element array and punctuation also is its own element\n",
    "#python does not distingiush between \" and ' therfore do not worry about words that \n",
    "#contain ' and appear in \"\" \n",
    "split_raw_text = re.findall(\"[\\w']+|[.,!?;:-]\" , raw_text)\t\n",
    "\n",
    "# estimate vocab size\n",
    "vocab = set(split_raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size,filters='#&()*+-/<=>@[\\]^_{|}~', lower=False)\n",
    "\n",
    "#fit the tokenizer on the text \n",
    "tokenizer.fit_on_texts(raw_text)\n",
    "\n",
    "# integer encode input and output\n",
    "encoded_input = tokenizer.texts_to_matrix(input_text, mode='count')\n",
    "encoded_output = tokenizer.texts_to_matrix(output_text, mode='count')\n",
    "\n",
    "# shape of Input should be: (Length of dataset, length of the largest sent)\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
