{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processor\n",
    "\n",
    "This file does the neural network business end. This'll have to be run every time we want to generate a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import math\n",
    "# from numpy.random import rand\n",
    "# from numpy.random import shuffle\n",
    "from pickle import load\n",
    "# from pickle import dump\n",
    "# import re\n",
    "# import os, sys, glob\n",
    "# #Don't run these imports on your local machine!\n",
    "import tensorflow as tf\n",
    "# #Keras imports\n",
    "# from keras.layers import LSTM, Dense, Activation, Input\n",
    "# from keras import optimizers\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Tokenize\n",
    "First, we load the datasets using our load_sentences method from above. We are going to load the full dataset (so we can calculate vocab and max_length sizes), and the train and test data.\n",
    "\n",
    "Next, we tokenize the data. Tokenization is the process of mapping words to integers. We are actually going to create separate tokenizers for our input and response data. Why? Because right now, that makes the code run. We can experiment with using one tokenizer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train entries:  27000\n",
      "Test entries:  3000\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Load data\n",
    "######################\n",
    "def load_sentences(filename):\n",
    "    return load(open(filename, \"rb\"))\n",
    "\n",
    "filepath = \"../../resources/\"\n",
    "dataset = load_sentences(filepath + \"utt-resp-both.pkl\")\n",
    "train = load_sentences(filepath + \"utt-resp-train.pkl\")\n",
    "test = load_sentences(filepath + \"utt-resp-test.pkl\")\n",
    "print(\"Train entries: \", len(train))\n",
    "print(\"Test entries: \",  len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Tokenizer methods\n",
    "######################\n",
    "#create and fit a tokenizer on the given lines\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "#get the max length of all phrases\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance vocabulary size: 9770\n",
      "Utterance max length: 83\n",
      "Response vocabulary size: 8164\n",
      "Response max length: 88\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Tokenize\n",
    "######################\n",
    "#create tokenizers\n",
    "utterance_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "response_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "\n",
    "#define vocabulary sizes\n",
    "utterance_vocab_size = len(utterance_tokenizer.word_index) + 1\n",
    "response_vocab_size = len(response_tokenizer.word_index) + 1\n",
    "\n",
    "#define max_lengths\n",
    "utterance_length = max_length(dataset[:, 0])\n",
    "response_length = max_length(dataset[:, 1])\n",
    "\n",
    "#print some statistics\n",
    "print(\"Utterance vocabulary size: %d\" % utterance_vocab_size)\n",
    "print(\"Utterance max length: %d\" % utterance_length)\n",
    "print(\"Response vocabulary size: %d\" % response_vocab_size)\n",
    "print(\"Response max length: %d\" % response_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "We need to encode each utterance-response sequence to integers, and pad each encoding to the maximum phrase length (so that every sequence of encoded integers is the same length).\n",
    "\n",
    "We need the encodings to be the same length because we are going to use a word embedding for the input sequences and one hot encode the output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Encoding methods\n",
    "######################\n",
    "#this method encodes the lines and pads them to the max length\n",
    "def encode_input(tokenizer, length, lines):\n",
    "    encoding = tokenizer.texts_to_sequences(lines)\n",
    "    encoding = pad_sequences(encoding, maxlen=length, padding=\"post\")\n",
    "    return encoding\n",
    "#this method one-hot encodes the output (responses). \n",
    "#we do this because we want the model to predict the probability of each word in the vocabulary as an output.\n",
    "\n",
    "def old_encode_output(sequences, vocab_size):\n",
    "    # THIS IS THE OLD OUTPUT ENCODING METHOD. IT HAS ERRORS BUT IT'S TRYING ITS BEST\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        #to_categorical converts a class vector (integers) to binary class matrix\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "#     print(sequences)\n",
    "#     print(vocab_size)\n",
    "#     sequences = sequences[:100]\n",
    "    output_array = np.empty([sequences.shape[0], sequences.shape[1], vocab_size])\n",
    "#     output_array = array(to_categorical(sequences[:1], num_classes=vocab_size))\n",
    "#     print(output_array.shape)\n",
    "#     output_array = list()\n",
    "    index = 0\n",
    "#     for sequence in sequences[1:]:\n",
    "    for sequence in sequences:\n",
    "#         if(index % 1000 == 0): print(index)\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "#         np.append(output_array, encoded)\n",
    "        output_array[index] = encoded\n",
    "        index += 1\n",
    "#         output_array.append(encoded)\n",
    "#     output_array = array(output_array)\n",
    "#     print(output_array.shape)\n",
    "#     print(sequences.shape[0])\n",
    "#     print(sequences.shape[1])\n",
    "#     output = output_array.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utterance:\n",
      "Training response (input):\n",
      "Training response (output):\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7fc1f100ae2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpiece_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpiece_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpieces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# train_response = encode_output(train_response, response_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Encode data\n",
    "######################\n",
    "#training data\n",
    "print(\"Training utterance:\")\n",
    "train_utterance = encode_input(utterance_tokenizer, utterance_length, train[:, 1])\n",
    "print(\"Training response (input):\")\n",
    "train_response = encode_input(response_tokenizer, response_length, train[:, 0])\n",
    "print(\"Training response (output):\")\n",
    "np.random.shuffle(train_response)    \n",
    "offset = 0\n",
    "piece_size = 1000\n",
    "pieces = list() # TURN THIS INTO A NUMPY ARRAY IT'S ADDING TO\n",
    "for i in range(5):\n",
    "    pieces.append(encode_output(train_response[offset:(offset + piece_size)], response_vocab_size))\n",
    "    offset += piece_size\n",
    "train_response = array(pieces)\n",
    "# train_response = encode_output(train_response, response_vocab_size)\n",
    "print(\"Training done.\")\n",
    "# #test data\n",
    "print(\"Testing utterance:\")\n",
    "test_utterance = encode_input(utterance_tokenizer, utterance_length, test[:, 1]) # BREAKS HERE\n",
    "print(\"Testing response (input):\")\n",
    "test_response = encode_input(response_tokenizer, response_length, test[:, 0])\n",
    "print(\"Testing response (output):\")\n",
    "test_response = encode_output(test_response, response_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "We will create an encoder-decoder LSTM.\n",
    "\n",
    "# What is a timestep?\n",
    "A timestep is a Keras thing. Here is the formal definition:\n",
    "\n",
    "The specified number of timesteps defines the number of input variables (X) used to predict the next time step (y).\n",
    "\n",
    "So, basically: A timestep is the \"memory\" of an LSTM- it's many inputs we are remembering (I think). In this case, we are using the max_length of an utterance/response as our timestep. This means that for every predicted word, we are taking into account every other word we have predicted so far. Likewise, when we train, we are learning weights for a word based on every previous word in a sentence (this is what we want for an encoder-decoder model!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Methods to create model\n",
    "######################\n",
    "#this method creates a model based on the given inputs.\n",
    "def create_model(input_vocab, output_vocab, input_timesteps, output_timesteps, n_units):\n",
    "    model = Sequential() #we are doing seq2seq \n",
    "    model.add(Embedding(input_vocab, n_units, input_length=input_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(output_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(output_vocab, activation=\"softmax\")))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 83, 256)           2501120   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 88, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 88, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 88, 8164)          2098148   \n",
      "=================================================================\n",
      "Total params: 5,649,892\n",
      "Trainable params: 5,649,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Create and compile model\n",
    "######################\n",
    "#We can change the number of hidden units (right now its 256)\n",
    "#increasing the number of hidden units will increase performance and training time\n",
    "#We can change the loss function (right now its categorical_crossentropy)\n",
    "#I also create a file called model.png that shows the shape of the model\n",
    "#I thought we might want to use the image for our final presentation :)\n",
    "model = create_model(utterance_vocab_size, response_vocab_size, utterance_length, response_length, 256)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file=\"model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Right now I'm using 30 epochs and a batch_size of 64. We can always up the number of epochs if we aren't getting good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      " - 123s - loss: 3.0537 - acc: 0.8797 - val_loss: 0.9317 - val_acc: 0.8988\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93171, saving model to model.test5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 83, 256)           2501120   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 88, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 88, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 88, 8164)          2098148   \n",
      "=================================================================\n",
      "Total params: 5,649,892\n",
      "Trainable params: 5,649,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filename= \"model.test5\"\n",
    "numEpochs = 50 #30 default\n",
    "batchSize = 64 #64 default\n",
    "checkpoint = ModelCheckpoint(filename, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "model.fit(train_utterance, train_response, epochs=numEpochs, batch_size=batchSize, validation_data=(test_utterance, test_response), callbacks=[checkpoint], verbose=2) # best performance with 40 epochs, 64 batch size\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload the datasets (just in case)\n",
    "dataset = load_sentences(\"utt-resp-both.pkl\")\n",
    "train = load_sentences(\"utt-resp-train.pkl\")\n",
    "test = load_sentences(\"utt-resp-test.pkl\")\n",
    "#create tokenizers\n",
    "utterance_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "response_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "#define vocabulary sizes\n",
    "utterance_vocab_size = len(utterance_tokenizer.word_index) + 1\n",
    "response_vocab_size = len(response_tokenizer.word_index) + 1\n",
    "#define max_lengths\n",
    "utterance_length = max_length(dataset[:, 0])\n",
    "response_length = max_length(dataset[:, 1])\n",
    "#datasets\n",
    "train_utt = encode_input(utterance_tokenizer, utterance_length, train[:, 1])\n",
    "test_utt = encode_input(utterance_tokenizer, utterance_length, train[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Evaluation methods\n",
    "######################\n",
    "#reverse-lookup a word in the tokenizer \n",
    "def get_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "#we will need to perform this reverse-lookup for every word in a predicted sequence\n",
    "#this method returns the prediction in words (not integers)\n",
    "def get_prediction(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = get_word(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return \" \".join(target)\n",
    "#we need to repeat the prediction for every utterance in the test dataset\n",
    "#we then compare our prediction to the actual response\n",
    "#I'm using a BLEU score to compare these quantitatively, but if we get a low BLEU score I wouldn't be surprised.\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = get_prediction(model, utterance_tokenizer, source)\n",
    "        raw_target, raw_source = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_source, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on training data:\n",
      "src=[sd\tThat was a great one.], target=[sd\tI'll have to write that down.], predicted=[]\n",
      "src=[^2\tThat's your weekend.], target=[sd\tand that's kind of the our social for the,], predicted=[]\n",
      "src=[sd\tand then } she had to go back in a couple of times,], target=[b\tYeah.], predicted=[]\n",
      "src=[b\tYeah.], target=[sd\twhen, previous secretary of, educa , Bennett, I think his name was, became the drug czar, for, President Bush, he was going to focus on this area and do something about it], predicted=[]\n",
      "src=[sv\tWell, you are not from that area originally, I can tell.], target=[sd\tBut, I didn't.], predicted=[]\n",
      "src=[b\tHuh uh.], target=[sd\tand she stays home, too, also.], predicted=[]\n",
      "src=[sv\tbut sometimes, it's baby sitting.], target=[b\tYeah.], predicted=[]\n",
      "src=[^h\tWell, let's see,], target=[sd\tand I feel like a native.], predicted=[]\n",
      "src=[sd\tI never had known anyone to play one before.], target=[b\tUh huh.], predicted=[]\n",
      "src=[sd\tNow spaghetti's such an easy one.], target=[sd\tI don't usually just pass on any recipe that's got more than five or six steps to it because I just know I'll never take the time to do it.], predicted=[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-05747346e702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#evalute on training data (this should be pretty good)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model on training data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterance_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_utt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#evaluate on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model on test data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-88bbf0218f1d>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, sources, raw_dataset)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutterance_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mraw_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-88bbf0218f1d>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(model, tokenizer, source)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintegers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-88bbf0218f1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintegers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Evaluate\n",
    "######################\n",
    "model = load_model(\"model.test5\")\n",
    "#evalute on training data (this should be pretty good)\n",
    "print(\"Model on training data:\")\n",
    "evaluate_model(model, utterance_tokenizer, train_utt, train)\n",
    "#evaluate on test data\n",
    "print(\"Model on test data:\")\n",
    "evaluate_model(model, utterance_tokenizer, test_utt, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
